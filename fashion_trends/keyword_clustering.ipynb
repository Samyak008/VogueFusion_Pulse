{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing seasons:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing season: Spring 2020\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing seasons:   0%|          | 0/25 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Size must be positive (size must be positive)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 134\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    133\u001b[0m     folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 134\u001b[0m     \u001b[43mprocess_season_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m, in \u001b[0;36mprocess_season_images\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m    104\u001b[0m images, filenames \u001b[38;5;241m=\u001b[39m load_images_from_folder(season_folder)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating labels for images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m labels \u001b[38;5;241m=\u001b[39m generate_image_labels(images)\n",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input image array is empty. Check the folder for valid image files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(images, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\djsma\\anaconda3\\envs\\tensor-py310\\lib\\site-packages\\keras\\applications\\resnet.py:521\u001b[0m, in \u001b[0;36mResNet50\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m     x \u001b[38;5;241m=\u001b[39m stack1(x, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m6\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stack1(x, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m3\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ResNet(\n\u001b[0;32m    522\u001b[0m     stack_fn,\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet50\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    526\u001b[0m     include_top,\n\u001b[0;32m    527\u001b[0m     weights,\n\u001b[0;32m    528\u001b[0m     input_tensor,\n\u001b[0;32m    529\u001b[0m     input_shape,\n\u001b[0;32m    530\u001b[0m     pooling,\n\u001b[0;32m    531\u001b[0m     classes,\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    533\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\djsma\\anaconda3\\envs\\tensor-py310\\lib\\site-packages\\keras\\applications\\resnet.py:238\u001b[0m, in \u001b[0;36mResNet\u001b[1;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         file_hash \u001b[38;5;241m=\u001b[39m WEIGHTS_HASHES[model_name][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    232\u001b[0m     weights_path \u001b[38;5;241m=\u001b[39m data_utils\u001b[38;5;241m.\u001b[39mget_file(\n\u001b[0;32m    233\u001b[0m         file_name,\n\u001b[0;32m    234\u001b[0m         BASE_WEIGHTS_PATH \u001b[38;5;241m+\u001b[39m file_name,\n\u001b[0;32m    235\u001b[0m         cache_subdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    236\u001b[0m         file_hash\u001b[38;5;241m=\u001b[39mfile_hash,\n\u001b[0;32m    237\u001b[0m     )\n\u001b[1;32m--> 238\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(weights)\n",
      "File \u001b[1;32mc:\\Users\\djsma\\anaconda3\\envs\\tensor-py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\djsma\\anaconda3\\envs\\tensor-py310\\lib\\site-packages\\h5py\\_hl\\attrs.py:67\u001b[0m, in \u001b[0;36mAttributeManager.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     63\u001b[0m dtype \u001b[38;5;241m=\u001b[39m attr\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Do this first, as we'll be fiddling with the dtype for top-level\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# array types\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m htype \u001b[38;5;241m=\u001b[39m \u001b[43mh5t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# NumPy doesn't support top-level array types, so we have to \"fake\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# the correct type and shape for the array.  For example, consider\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# attr.shape == (5,) and attr.dtype == '(3,)f'. Then:\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39msubdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1669\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1693\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1731\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1544\u001b[0m, in \u001b[0;36mh5py.h5t._c_string\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Size must be positive (size must be positive)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = (224, 224)  # Input size for ResNet\n",
    "NUM_CLUSTERS = 5  # Change based on the number of trends you want to find\n",
    "SEASON_TUPLE = (\n",
    "    'Spring 2020', 'Summer 2020', 'Fall 2020', 'Autumn 2020', 'Winter 2020',\n",
    "    'Spring 2021', 'Summer 2021', 'Fall 2021', 'Autumn 2021', 'Winter 2021',\n",
    "    'Spring 2022', 'Summer 2022', 'Fall 2022', 'Autumn 2022', 'Winter 2022',\n",
    "    'Spring 2023', 'Summer 2023', 'Fall 2023', 'Autumn 2023', 'Winter 2023',\n",
    "    'Spring 2024', 'Summer 2024', 'Fall 2024', 'Autumn 2024', 'Winter 2024',\n",
    ")\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    img = load_img(file_path, target_size=IMAGE_SIZE)\n",
    "                    img_array = img_to_array(img)\n",
    "                    img_array = preprocess_input(img_array)\n",
    "                    images.append(img_array)\n",
    "                    filenames.append(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {file_path}: {e}\")\n",
    "    if not images:\n",
    "        print(f\"No valid images found in folder: {folder_path}\")\n",
    "    return np.array(images), filenames\n",
    "\n",
    "# Function to extract features using ResNet50\n",
    "def extract_features(images):\n",
    "    if images.size == 0:\n",
    "        raise ValueError(\"The input image array is empty. Check the folder for valid image files.\")\n",
    "    \n",
    "    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features = model.predict(images, batch_size=32, verbose=1)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Function to label images with keywords\n",
    "def generate_image_labels(images):\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    labels = []\n",
    "    for img_array in tqdm(images, desc=\"Labeling images\"):\n",
    "        predictions = model.predict(np.expand_dims(img_array, axis=0))\n",
    "        decoded_predictions = decode_predictions(predictions, top=5)\n",
    "        keywords = [pred[1] for pred in decoded_predictions[0]]  # Get top 5 keywords\n",
    "        labels.append(keywords)\n",
    "    return labels\n",
    "\n",
    "# Function to perform clustering\n",
    "def cluster_features(features, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    return labels, kmeans\n",
    "\n",
    "# Function to visualize clusters\n",
    "def visualize_clusters(features, labels, filenames):\n",
    "    pca = PCA(n_components=50)\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_features = tsne.fit_transform(reduced_features)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in np.unique(labels):\n",
    "        indices = np.where(labels == label)\n",
    "        plt.scatter(tsne_features[indices, 0], tsne_features[indices, 1], label=f'Cluster {label}')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Clusters of Dresses')\n",
    "    plt.show()\n",
    "\n",
    "    # Save images grouped by cluster\n",
    "    for label in np.unique(labels):\n",
    "        cluster_folder = f'cluster_{label}'\n",
    "        os.makedirs(cluster_folder, exist_ok=True)\n",
    "        for idx in np.where(labels == label)[0]:\n",
    "            src_path = filenames[idx]\n",
    "            dst_path = os.path.join(cluster_folder, os.path.basename(src_path))\n",
    "            os.link(src_path, dst_path)\n",
    "\n",
    "# Function to process each season separately\n",
    "def process_season_images(folder_path):\n",
    "    results = []\n",
    "    for season in tqdm(SEASON_TUPLE, desc=\"Processing seasons\"):\n",
    "        season_folder = os.path.join(folder_path, season)\n",
    "        if os.path.exists(season_folder):\n",
    "            print(f\"\\nProcessing season: {season}\")\n",
    "            images, filenames = load_images_from_folder(season_folder)\n",
    "\n",
    "            print(\"Extracting features...\")\n",
    "            features = extract_features(images)\n",
    "\n",
    "            print(\"Generating labels for images...\")\n",
    "            labels = generate_image_labels(images)\n",
    "\n",
    "            print(\"Clustering features...\")\n",
    "            cluster_labels, kmeans = cluster_features(features, NUM_CLUSTERS)\n",
    "\n",
    "            # Compile results\n",
    "            for i, filename in enumerate(filenames):\n",
    "                results.append({\n",
    "                    'Season': season,\n",
    "                    'Filename': filename,\n",
    "                    'Cluster': cluster_labels[i],\n",
    "                    'Keywords': \", \".join(labels[i])\n",
    "                })\n",
    "\n",
    "            print(f\"Done processing season: {season}!\")\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('seasonal_dress_clusters.csv', index=False)\n",
    "    print(\"Results saved to 'seasonal_dress_clusters.csv'.\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"images1\"\n",
    "    process_season_images(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
